{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e83e26",
   "metadata": {},
   "source": [
    "# Pandas vs Pyspark Fetaures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f6d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Pyspark vs Pandas\").getOrCreate()\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d26887e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eno        int64\n",
       "ename     object\n",
       "salary     int64\n",
       "deptno     int64\n",
       "doj       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file using Pandas\n",
    "df_pd = pd.read_csv(\"emp.csv\")\n",
    "# Dfeault datatypes\n",
    "df_pd.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abdcfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eno: string (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- deptno: string (nullable = true)\n",
      " |-- doj: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read CSV file using Pyspark\n",
    "df_spark = spark.read.csv(\"emp.csv\",header=True)\n",
    "# default datatypes\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01fc5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      " |-- doj: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Extract the schema based on file data values\n",
    "df_spark = spark.read.csv(\"emp.csv\",header=True,inferSchema=True)\n",
    "# default datatypes\n",
    "df_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78875592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "|eno| ename|salary|deptno|       doj|\n",
      "+---+------+------+------+----------+\n",
      "| 10| Amber|  1000|    10|2018-05-14|\n",
      "|  2|  Amar|  1000|    10|2017-03-22|\n",
      "|  8| Sagar|  1000|    20|2020-11-30|\n",
      "|  4|Thomas|  1000|    10|2019-08-18|\n",
      "|  5|  Jade|  2000|    10|2021-02-10|\n",
      "|  6| Smith|  1000|    10|2016-12-01|\n",
      "|  7|Robert|  3000|    20|2023-06-25|\n",
      "|  3|  Nick|  1000|    30|2015-09-15|\n",
      "|  9| Naman|  4000|    30|2020-01-20|\n",
      "|  1| Vikas|  1000|    30|2017-10-03|\n",
      "| 11|Dennis| 41000|    30|2024-01-05|\n",
      "| 12| Karan|  2200|    20|2018-06-16|\n",
      "| 13|Simran|  1500|    10|2019-12-19|\n",
      "| 14|  Amit|  2000|    30|2022-04-09|\n",
      "| 15|  Neha|  1700|    20|2021-07-22|\n",
      "| 16| Rahul|  1200|    10|2016-03-28|\n",
      "| 17| Divya|  3000|    30|2015-11-11|\n",
      "| 18|Rakesh|  1800|    10|2020-09-04|\n",
      "| 19|Monica|  2500|    20|2023-08-07|\n",
      "| 20|  John|  2300|    30|2018-10-29|\n",
      "+---+------+------+------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6008162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file using Pyspark\n",
    "#df_spark1 = spark.read.option(\"emp.csv\",header=True)\n",
    "# default datatypes\n",
    "#df_spark.printSchema()\n",
    "df_sprk = spark.read .option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(\"emp.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "44f2c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|eno|salary|deptno|\n",
      "+---+------+------+\n",
      "| 10|  1000|    10|\n",
      "|  2|  1000|    10|\n",
      "|  8|  1000|    20|\n",
      "|  4|  1000|    10|\n",
      "|  5|  2000|    10|\n",
      "|  6|  1000|    10|\n",
      "|  7|  3000|    20|\n",
      "|  3|  1000|    30|\n",
      "|  9|  4000|    30|\n",
      "|  1|  1000|    30|\n",
      "| 11| 41000|    30|\n",
      "| 12|  2200|    20|\n",
      "| 13|  1500|    10|\n",
      "| 14|  2000|    30|\n",
      "| 15|  1700|    20|\n",
      "| 16|  1200|    10|\n",
      "| 17|  3000|    30|\n",
      "| 18|  1800|    10|\n",
      "| 19|  2500|    20|\n",
      "| 20|  2300|    30|\n",
      "+---+------+------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Read only sepcific columns\n",
    "df_pd = pd.read_csv(\"emp.csv\")\n",
    "df_sprk = spark.read.csv(\"emp.csv\",header=True,inferSchema=True).select(\"eno\",\"salary\",\"deptno\")\n",
    "df_sprk.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1fbb07d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eno</th>\n",
       "      <th>deptno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    eno  deptno\n",
       "10   11      30\n",
       "11   12      20\n",
       "12   13      10\n",
       "13   14      30\n",
       "14   15      20\n",
       "15   16      10\n",
       "16   17      30\n",
       "17   18      10\n",
       "18   19      20\n",
       "19   20      30"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read specific row from 11th to 20th rows\n",
    "df_pd.head(20).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c63d4586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "|eno| ename|salary|deptno|       doj|\n",
      "+---+------+------+------+----------+\n",
      "| 11|Dennis| 41000|    30|2024-01-05|\n",
      "| 12| Karan|  2200|    20|2018-06-16|\n",
      "| 13|Simran|  1500|    10|2019-12-19|\n",
      "| 14|  Amit|  2000|    30|2022-04-09|\n",
      "| 15|  Neha|  1700|    20|2021-07-22|\n",
      "| 16| Rahul|  1200|    10|2016-03-28|\n",
      "| 17| Divya|  3000|    30|2015-11-11|\n",
      "| 18|Rakesh|  1800|    10|2020-09-04|\n",
      "| 19|Monica|  2500|    20|2023-08-07|\n",
      "| 20|  John|  2300|    30|2018-10-29|\n",
      "+---+------+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read specific row from 11th to 20th rows\n",
    "df_sprk.limit(20).subtract(df_sprk.limit(10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "669d9398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eno: integer (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- deptno: integer (nullable = true)\n",
      " |-- doj: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sprk.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a5da700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data in to another file in pandas\n",
    "#df_pd.to_csv(\"emp_output_pd.csv\")\n",
    "\n",
    "# Write the data in to another file in pyspark\n",
    "df_sprk.write.mode(\"overwrite\").option(\"header\",True).csv(\"pysprk_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "31ac2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a zipped(compressed) file in pyspark\n",
    "df_sp = spark.read .option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").option(\"compression\",\"gzip\").csv(\"emp_bkp.csv.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906f4deb",
   "metadata": {},
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baddb9ba",
   "metadata": {},
   "source": [
    "# Sorting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b21506ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting by salary in pandas\n",
    "df_pd = df_pd.sort_values(by='salary',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6b19d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "|eno| ename|salary|deptno|       doj|\n",
      "+---+------+------+------+----------+\n",
      "| 11|Dennis| 41000|    30|2024-01-05|\n",
      "|  9| Naman|  4000|    30|2020-01-20|\n",
      "| 36|  Ritu|  3200|    20|2023-05-01|\n",
      "|  7|Robert|  3000|    20|2023-06-25|\n",
      "| 17| Divya|  3000|    30|2015-11-11|\n",
      "| 35| Seema|  3000|    10|2021-10-15|\n",
      "| 37| Kunal|  2800|    30|2015-08-22|\n",
      "| 21| Sneha|  2700|    10|2022-12-14|\n",
      "| 38|  Alok|  2700|    10|2019-03-02|\n",
      "| 39|Poonam|  2600|    20|2022-02-05|\n",
      "| 19|Monica|  2500|    20|2023-08-07|\n",
      "| 40|  Yash|  2500|    30|2020-07-19|\n",
      "| 33| Nidhi|  2400|    30|2016-05-12|\n",
      "| 20|  John|  2300|    30|2018-10-29|\n",
      "| 41|  Nina|  2300|    20|2018-12-28|\n",
      "| 12| Karan|  2200|    20|2018-06-16|\n",
      "| 42| Tarun|  2200|    10|2021-01-09|\n",
      "| 43| Geeta|  2100|    30|2017-11-14|\n",
      "| 32|  Ajay|  2100|    10|2020-06-26|\n",
      "| 34| Vivek|  2000|    20|2017-07-31|\n",
      "+---+------+------+------+----------+\n",
      "only showing top 20 rows\n",
      "+---+------+------+------+----------+\n",
      "|eno| ename|salary|deptno|       doj|\n",
      "+---+------+------+------+----------+\n",
      "| 22| David|  1000|    20|2017-02-17|\n",
      "| 10| Amber|  1000|    10|2018-05-14|\n",
      "|  8| Sagar|  1000|    20|2020-11-30|\n",
      "|  4|Thomas|  1000|    10|2019-08-18|\n",
      "|  6| Smith|  1000|    10|2016-12-01|\n",
      "|  3|  Nick|  1000|    30|2015-09-15|\n",
      "|  1| Vikas|  1000|    30|2017-10-03|\n",
      "|  2|  Amar|  1000|    10|2017-03-22|\n",
      "| 30|Suresh|  1100|    10|2018-09-09|\n",
      "| 16| Rahul|  1200|    10|2016-03-28|\n",
      "| 29| Priya|  1200|    20|2023-02-11|\n",
      "| 28| Mohan|  1300|    10|2015-04-17|\n",
      "| 27|Kavita|  1400|    30|2019-01-06|\n",
      "| 13|Simran|  1500|    10|2019-12-19|\n",
      "| 23| Anita|  1500|    30|2019-05-21|\n",
      "| 26| Harsh|  1600|    10|2021-09-25|\n",
      "| 47| Payal|  1600|    20|2015-10-10|\n",
      "| 15|  Neha|  1700|    20|2021-07-22|\n",
      "| 46| Manoj|  1700|    30|2019-07-24|\n",
      "| 18|Rakesh|  1800|    10|2020-09-04|\n",
      "+---+------+------+------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Sorting by salary in pyspark\n",
    "df_sp.orderBy(df_sp.salary.desc()).show()\n",
    "df_sp.orderBy(df_sp.salary.asc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8435630c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------+----------+\n",
      "|eno| ename|salary|deptno|       doj|\n",
      "+---+------+------+------+----------+\n",
      "| 35| Seema|  3000|    10|2021-10-15|\n",
      "| 21| Sneha|  2700|    10|2022-12-14|\n",
      "| 38|  Alok|  2700|    10|2019-03-02|\n",
      "| 42| Tarun|  2200|    10|2021-01-09|\n",
      "| 32|  Ajay|  2100|    10|2020-06-26|\n",
      "|  5|  Jade|  2000|    10|2021-02-10|\n",
      "| 44| Ashok|  1900|    10|2016-06-08|\n",
      "| 24| Vinod|  1800|    10|2016-07-30|\n",
      "| 18|Rakesh|  1800|    10|2020-09-04|\n",
      "| 26| Harsh|  1600|    10|2021-09-25|\n",
      "| 13|Simran|  1500|    10|2019-12-19|\n",
      "| 28| Mohan|  1300|    10|2015-04-17|\n",
      "| 16| Rahul|  1200|    10|2016-03-28|\n",
      "| 30|Suresh|  1100|    10|2018-09-09|\n",
      "| 10| Amber|  1000|    10|2018-05-14|\n",
      "|  4|Thomas|  1000|    10|2019-08-18|\n",
      "|  2|  Amar|  1000|    10|2017-03-22|\n",
      "|  6| Smith|  1000|    10|2016-12-01|\n",
      "| 36|  Ritu|  3200|    20|2023-05-01|\n",
      "|  7|Robert|  3000|    20|2023-06-25|\n",
      "+---+------+------+------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# Sorting by deptno in ascending and salary in descending order in pyspark\n",
    "df_sp.orderBy(df_sp.deptno.asc(),df_sp.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9938b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PySpark version: 4.0.0\n",
      " Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\" PySpark version:\", pyspark.__version__)\n",
    "print(\" Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9367afb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|deptno|sum_Values|\n",
      "+------+----------+\n",
      "|    20|     30000|\n",
      "|    10|     30900|\n",
      "|    30|     68700|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by one colums\n",
    "df = spark.read .option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"delimiter\", \",\").csv(\"emp.csv\")\n",
    "df.groupBy('deptno').agg(F.sum('salary').alias('sum_Values')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8da1d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+----------+\n",
      "|     city|deptid|sum_Values|\n",
      "+---------+------+----------+\n",
      "|  Chennai|    40|     65000|\n",
      "|   Mumbai|    20|     60000|\n",
      "|Bengaluru|    50|     48000|\n",
      "|     Pune|    20|     49000|\n",
      "|    Delhi|    20|     62000|\n",
      "|   Mumbai|    30|     59000|\n",
      "|Hyderabad|    50|     61000|\n",
      "|Ahmedabad|    30|     61000|\n",
      "|Bengaluru|    10|    110000|\n",
      "|  Kolkata|    10|    107000|\n",
      "|  Chennai|    60|     66000|\n",
      "|   Jaipur|    50|     47000|\n",
      "|     Pune|    30|     57000|\n",
      "|    Delhi|    40|    130000|\n",
      "|Hyderabad|    30|     60000|\n",
      "|    Surat|    20|     56000|\n",
      "|   Mumbai|    10|     55000|\n",
      "+---------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by multiple columns\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(\"emp_details.json\")\n",
    "#df.show()\n",
    "df.groupBy('city','deptid').agg(F.sum('salary').alias('sum_Values')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cee24e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+-------+\n",
      "|deptid|   sum|   mean|    avg|\n",
      "+------+------+-------+-------+\n",
      "|    50|156000|52000.0|52000.0|\n",
      "|    10|272000|54400.0|54400.0|\n",
      "|    30|237000|59250.0|59250.0|\n",
      "|    20|227000|56750.0|56750.0|\n",
      "|    60| 66000|66000.0|66000.0|\n",
      "|    40|195000|65000.0|65000.0|\n",
      "+------+------+-------+-------+\n",
      "\n",
      "+---------+------+---+------+\n",
      "|     city|deptid|eno|salary|\n",
      "+---------+------+---+------+\n",
      "|   Mumbai|    10|101| 55000|\n",
      "|    Delhi|    20|102| 62000|\n",
      "|Bengaluru|    10|103| 58000|\n",
      "|Hyderabad|    30|104| 60000|\n",
      "|     Pune|    20|105| 49000|\n",
      "|  Kolkata|    10|106| 53000|\n",
      "|  Chennai|    40|107| 65000|\n",
      "|Ahmedabad|    30|108| 61000|\n",
      "|   Jaipur|    50|109| 47000|\n",
      "|    Surat|    20|110| 56000|\n",
      "|   Mumbai|    30|111| 59000|\n",
      "|    Delhi|    40|112| 63000|\n",
      "|Bengaluru|    10|113| 52000|\n",
      "|Hyderabad|    50|114| 61000|\n",
      "|  Chennai|    60|115| 66000|\n",
      "|     Pune|    30|116| 57000|\n",
      "|  Kolkata|    10|117| 54000|\n",
      "|   Mumbai|    20|118| 60000|\n",
      "|    Delhi|    40|119| 67000|\n",
      "|Bengaluru|    50|120| 48000|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group by one colun but multile agrregate function\n",
    "df.groupBy('deptid').agg( F.sum('salary').alias('sum'), F.mean('salary').alias('mean'), F.avg('salary').alias('avg') ).show()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3514c1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|     city|deptid|eno|salary|\n",
      "+---------+------+---+------+\n",
      "|    Delhi|    20|102| 62000|\n",
      "|  Chennai|    40|107| 65000|\n",
      "|Ahmedabad|    30|108| 61000|\n",
      "|    Delhi|    40|112| 63000|\n",
      "|Hyderabad|    50|114| 61000|\n",
      "|  Chennai|    60|115| 66000|\n",
      "|    Delhi|    40|119| 67000|\n",
      "+---------+------+---+------+\n",
      "\n",
      "+------+------------+\n",
      "|deptid|total_salary|\n",
      "+------+------------+\n",
      "|    50|       61000|\n",
      "|    30|       61000|\n",
      "|    20|       62000|\n",
      "|    60|       66000|\n",
      "|    40|      195000|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter before grouping\n",
    "filtered = df.filter(df.salary > 60000)\n",
    "filtered.show()\n",
    "filtered.groupBy('deptid').agg(F.sum('salary').alias('total_salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24a9bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|deptid|total_salary|\n",
      "+------+------------+\n",
      "|    60|       66000|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter after grouping (HAVING)\n",
    "df.groupBy('deptid').agg(F.sum('salary').alias('total_salary')).filter('total_salary < 70000').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da98428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joiner in Pyspark\n",
    "df1 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(\"emp_details.json\")\n",
    "df2 = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").json(\"department.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1912dee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|  deptname|deptno|\n",
      "+----------+------+\n",
      "|   Finance|    10|\n",
      "|        HR|    20|\n",
      "|        IT|    30|\n",
      "|Operations|    40|\n",
      "| Marketing|    50|\n",
      "|     Legal|    70|\n",
      "|     Admin|    80|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "13012e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+----------+------+\n",
      "|     city|deptid|eno|salary|  deptname|deptno|\n",
      "+---------+------+---+------+----------+------+\n",
      "|   Mumbai|    10|101| 55000|   Finance|    10|\n",
      "|    Delhi|    20|102| 62000|        HR|    20|\n",
      "|Bengaluru|    10|103| 58000|   Finance|    10|\n",
      "|Hyderabad|    30|104| 60000|        IT|    30|\n",
      "|     Pune|    20|105| 49000|        HR|    20|\n",
      "|  Kolkata|    10|106| 53000|   Finance|    10|\n",
      "|  Chennai|    40|107| 65000|Operations|    40|\n",
      "|Ahmedabad|    30|108| 61000|        IT|    30|\n",
      "|   Jaipur|    50|109| 47000| Marketing|    50|\n",
      "|    Surat|    20|110| 56000|        HR|    20|\n",
      "|   Mumbai|    30|111| 59000|        IT|    30|\n",
      "|    Delhi|    40|112| 63000|Operations|    40|\n",
      "|Bengaluru|    10|113| 52000|   Finance|    10|\n",
      "|Hyderabad|    50|114| 61000| Marketing|    50|\n",
      "|     Pune|    30|116| 57000|        IT|    30|\n",
      "|  Kolkata|    10|117| 54000|   Finance|    10|\n",
      "|   Mumbai|    20|118| 60000|        HR|    20|\n",
      "|    Delhi|    40|119| 67000|Operations|    40|\n",
      "|Bengaluru|    50|120| 48000| Marketing|    50|\n",
      "+---------+------+---+------+----------+------+\n",
      "\n",
      "+---------+------+---+------+----------+------+\n",
      "|     city|deptid|eno|salary|  deptname|deptno|\n",
      "+---------+------+---+------+----------+------+\n",
      "|   Mumbai|    10|101| 55000|   Finance|    10|\n",
      "|    Delhi|    20|102| 62000|        HR|    20|\n",
      "|Bengaluru|    10|103| 58000|   Finance|    10|\n",
      "|Hyderabad|    30|104| 60000|        IT|    30|\n",
      "|     Pune|    20|105| 49000|        HR|    20|\n",
      "|  Kolkata|    10|106| 53000|   Finance|    10|\n",
      "|  Chennai|    40|107| 65000|Operations|    40|\n",
      "|Ahmedabad|    30|108| 61000|        IT|    30|\n",
      "|   Jaipur|    50|109| 47000| Marketing|    50|\n",
      "|    Surat|    20|110| 56000|        HR|    20|\n",
      "|   Mumbai|    30|111| 59000|        IT|    30|\n",
      "|    Delhi|    40|112| 63000|Operations|    40|\n",
      "|Bengaluru|    10|113| 52000|   Finance|    10|\n",
      "|Hyderabad|    50|114| 61000| Marketing|    50|\n",
      "|  Chennai|    60|115| 66000|      NULL|  NULL|\n",
      "|     Pune|    30|116| 57000|        IT|    30|\n",
      "|  Kolkata|    10|117| 54000|   Finance|    10|\n",
      "|   Mumbai|    20|118| 60000|        HR|    20|\n",
      "|    Delhi|    40|119| 67000|Operations|    40|\n",
      "|Bengaluru|    50|120| 48000| Marketing|    50|\n",
      "+---------+------+---+------+----------+------+\n",
      "\n",
      "+---------+------+----+------+----------+------+\n",
      "|     city|deptid| eno|salary|  deptname|deptno|\n",
      "+---------+------+----+------+----------+------+\n",
      "|  Kolkata|    10| 117| 54000|   Finance|    10|\n",
      "|Bengaluru|    10| 113| 52000|   Finance|    10|\n",
      "|  Kolkata|    10| 106| 53000|   Finance|    10|\n",
      "|Bengaluru|    10| 103| 58000|   Finance|    10|\n",
      "|   Mumbai|    10| 101| 55000|   Finance|    10|\n",
      "|   Mumbai|    20| 118| 60000|        HR|    20|\n",
      "|    Surat|    20| 110| 56000|        HR|    20|\n",
      "|     Pune|    20| 105| 49000|        HR|    20|\n",
      "|    Delhi|    20| 102| 62000|        HR|    20|\n",
      "|     Pune|    30| 116| 57000|        IT|    30|\n",
      "|   Mumbai|    30| 111| 59000|        IT|    30|\n",
      "|Ahmedabad|    30| 108| 61000|        IT|    30|\n",
      "|Hyderabad|    30| 104| 60000|        IT|    30|\n",
      "|    Delhi|    40| 119| 67000|Operations|    40|\n",
      "|    Delhi|    40| 112| 63000|Operations|    40|\n",
      "|  Chennai|    40| 107| 65000|Operations|    40|\n",
      "|Bengaluru|    50| 120| 48000| Marketing|    50|\n",
      "|Hyderabad|    50| 114| 61000| Marketing|    50|\n",
      "|   Jaipur|    50| 109| 47000| Marketing|    50|\n",
      "|     NULL|  NULL|NULL|  NULL|     Legal|    70|\n",
      "+---------+------+----+------+----------+------+\n",
      "only showing top 20 rows\n",
      "+---------+------+---+------+----------+------+\n",
      "|     city|deptid|eno|salary|  deptname|deptno|\n",
      "+---------+------+---+------+----------+------+\n",
      "|   Mumbai|    10|101| 55000|   Finance|    10|\n",
      "|Bengaluru|    10|103| 58000|   Finance|    10|\n",
      "|  Kolkata|    10|106| 53000|   Finance|    10|\n",
      "|Bengaluru|    10|113| 52000|   Finance|    10|\n",
      "|  Kolkata|    10|117| 54000|   Finance|    10|\n",
      "|    Delhi|    20|102| 62000|        HR|    20|\n",
      "|     Pune|    20|105| 49000|        HR|    20|\n",
      "|    Surat|    20|110| 56000|        HR|    20|\n",
      "|   Mumbai|    20|118| 60000|        HR|    20|\n",
      "|Hyderabad|    30|104| 60000|        IT|    30|\n",
      "|Ahmedabad|    30|108| 61000|        IT|    30|\n",
      "|   Mumbai|    30|111| 59000|        IT|    30|\n",
      "|     Pune|    30|116| 57000|        IT|    30|\n",
      "|  Chennai|    40|107| 65000|Operations|    40|\n",
      "|    Delhi|    40|112| 63000|Operations|    40|\n",
      "|    Delhi|    40|119| 67000|Operations|    40|\n",
      "|   Jaipur|    50|109| 47000| Marketing|    50|\n",
      "|Hyderabad|    50|114| 61000| Marketing|    50|\n",
      "|Bengaluru|    50|120| 48000| Marketing|    50|\n",
      "|  Chennai|    60|115| 66000|      NULL|  NULL|\n",
      "+---------+------+---+------+----------+------+\n",
      "only showing top 20 rows\n",
      "+---------+------+---+------+\n",
      "|     city|deptid|eno|salary|\n",
      "+---------+------+---+------+\n",
      "|   Mumbai|    10|101| 55000|\n",
      "|    Delhi|    20|102| 62000|\n",
      "|Bengaluru|    10|103| 58000|\n",
      "|Hyderabad|    30|104| 60000|\n",
      "|     Pune|    20|105| 49000|\n",
      "|  Kolkata|    10|106| 53000|\n",
      "|  Chennai|    40|107| 65000|\n",
      "|Ahmedabad|    30|108| 61000|\n",
      "|   Jaipur|    50|109| 47000|\n",
      "|    Surat|    20|110| 56000|\n",
      "|   Mumbai|    30|111| 59000|\n",
      "|    Delhi|    40|112| 63000|\n",
      "|Bengaluru|    10|113| 52000|\n",
      "|Hyderabad|    50|114| 61000|\n",
      "|     Pune|    30|116| 57000|\n",
      "|  Kolkata|    10|117| 54000|\n",
      "|   Mumbai|    20|118| 60000|\n",
      "|    Delhi|    40|119| 67000|\n",
      "|Bengaluru|    50|120| 48000|\n",
      "+---------+------+---+------+\n",
      "\n",
      "+-------+------+---+------+\n",
      "|   city|deptid|eno|salary|\n",
      "+-------+------+---+------+\n",
      "|Chennai|    60|115| 66000|\n",
      "+-------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "emp_df = spark.read.json(\"emp_details.json\")\n",
    "dept_df = spark.read.json(\"department.json\")\n",
    "\n",
    "# INNER JOIN\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"inner\").show()\n",
    "\n",
    "# LEFT JOIN\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"left\").show()\n",
    "\n",
    "# RIGHT JOIN\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"right\").show()\n",
    "\n",
    "# FULL OUTER JOIN\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"full\").show()\n",
    "\n",
    "# LEFT SEMI (only employees with matching dept)\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"left_semi\").show()\n",
    "\n",
    "# LEFT ANTI (employees whose dept not found in dept_df)\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"left_anti\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "19d65ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+----------+------+\n",
      "|     city|deptid|eno|salary|  deptname|deptno|\n",
      "+---------+------+---+------+----------+------+\n",
      "|   Mumbai|    10|101| 55000|   Finance|    10|\n",
      "|    Delhi|    20|102| 62000|        HR|    20|\n",
      "|Bengaluru|    10|103| 58000|   Finance|    10|\n",
      "|Hyderabad|    30|104| 60000|        IT|    30|\n",
      "|     Pune|    20|105| 49000|        HR|    20|\n",
      "|  Kolkata|    10|106| 53000|   Finance|    10|\n",
      "|  Chennai|    40|107| 65000|Operations|    40|\n",
      "|Ahmedabad|    30|108| 61000|        IT|    30|\n",
      "|   Jaipur|    50|109| 47000| Marketing|    50|\n",
      "|    Surat|    20|110| 56000|        HR|    20|\n",
      "|   Mumbai|    30|111| 59000|        IT|    30|\n",
      "|    Delhi|    40|112| 63000|Operations|    40|\n",
      "|Bengaluru|    10|113| 52000|   Finance|    10|\n",
      "|Hyderabad|    50|114| 61000| Marketing|    50|\n",
      "|     Pune|    30|116| 57000|        IT|    30|\n",
      "|  Kolkata|    10|117| 54000|   Finance|    10|\n",
      "|   Mumbai|    20|118| 60000|        HR|    20|\n",
      "|    Delhi|    40|119| 67000|Operations|    40|\n",
      "|Bengaluru|    50|120| 48000| Marketing|    50|\n",
      "+---------+------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"inner\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fd1fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+\n",
      "|     city|deptid|eno|salary|\n",
      "+---------+------+---+------+\n",
      "|   Mumbai|    10|101| 55000|\n",
      "|    Delhi|    20|102| 62000|\n",
      "|Bengaluru|    10|103| 58000|\n",
      "|Hyderabad|    30|104| 60000|\n",
      "|     Pune|    20|105| 49000|\n",
      "|  Kolkata|    10|106| 53000|\n",
      "|  Chennai|    40|107| 65000|\n",
      "|Ahmedabad|    30|108| 61000|\n",
      "|   Jaipur|    50|109| 47000|\n",
      "|    Surat|    20|110| 56000|\n",
      "|   Mumbai|    30|111| 59000|\n",
      "|    Delhi|    40|112| 63000|\n",
      "|Bengaluru|    10|113| 52000|\n",
      "|Hyderabad|    50|114| 61000|\n",
      "|     Pune|    30|116| 57000|\n",
      "|  Kolkata|    10|117| 54000|\n",
      "|   Mumbai|    20|118| 60000|\n",
      "|    Delhi|    40|119| 67000|\n",
      "|Bengaluru|    50|120| 48000|\n",
      "+---------+------+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEFT SEMI (only employees with matching dept)\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"left_semi\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "280c5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+------+\n",
      "|   city|deptid|eno|salary|\n",
      "+-------+------+---+------+\n",
      "|Chennai|    60|115| 66000|\n",
      "+-------+------+---+------+\n",
      "\n",
      "+--------+------+\n",
      "|deptname|deptno|\n",
      "+--------+------+\n",
      "|   Legal|    70|\n",
      "|   Admin|    80|\n",
      "+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LEFT ANTI (employees whose dept not found in dept_df)\n",
    "emp_df.join(dept_df, emp_df.deptid == dept_df.deptno, \"left_anti\").show()\n",
    "dept_df.join(emp_df, emp_df.deptid == dept_df.deptno, \"left_anti\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9652ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---+------+----------+------+\n",
      "|     city|deptid|eno|salary|  deptname|deptno|\n",
      "+---------+------+---+------+----------+------+\n",
      "|   Mumbai|    10|101| 55000|   Finance|    10|\n",
      "|    Delhi|    20|102| 62000|        HR|    20|\n",
      "|Bengaluru|    10|103| 58000|   Finance|    10|\n",
      "|Hyderabad|    30|104| 60000|        IT|    30|\n",
      "|     Pune|    20|105| 49000|        HR|    20|\n",
      "|  Kolkata|    10|106| 53000|   Finance|    10|\n",
      "|  Chennai|    40|107| 65000|Operations|    40|\n",
      "|Ahmedabad|    30|108| 61000|        IT|    30|\n",
      "|   Jaipur|    50|109| 47000| Marketing|    50|\n",
      "|    Surat|    20|110| 56000|        HR|    20|\n",
      "|   Mumbai|    30|111| 59000|        IT|    30|\n",
      "|    Delhi|    40|112| 63000|Operations|    40|\n",
      "|Bengaluru|    10|113| 52000|   Finance|    10|\n",
      "|Hyderabad|    50|114| 61000| Marketing|    50|\n",
      "|     Pune|    30|116| 57000|        IT|    30|\n",
      "|  Kolkata|    10|117| 54000|   Finance|    10|\n",
      "|   Mumbai|    20|118| 60000|        HR|    20|\n",
      "|    Delhi|    40|119| 67000|Operations|    40|\n",
      "|Bengaluru|    50|120| 48000| Marketing|    50|\n",
      "+---------+------+---+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.join(other=dept_df,on=emp_df.deptid == dept_df.deptno,how=\"inner\").show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
