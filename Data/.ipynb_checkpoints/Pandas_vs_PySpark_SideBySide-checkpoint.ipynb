{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4763accc",
   "metadata": {},
   "source": [
    "\n",
    "# üß† Pandas vs PySpark ‚Äî CSV Reading & Operations (Side-by-Side Practice)\n",
    "\n",
    "This notebook shows **Pandas** and **PySpark** equivalents for reading and manipulating CSV files.\n",
    "\n",
    "üëâ You can run this locally on Jupyter after installing:\n",
    "```bash\n",
    "pip install pandas pyspark\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e149290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ emp.csv created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================\n",
    "# üìÅ Create a sample CSV file (emp.csv)\n",
    "# =============================================================\n",
    "\n",
    "csv_data = '''eno,ename,deptno,salary,doj\n",
    "1,Amit,10,50000,2021-01-10\n",
    "2,Neha,20,60000,2021-02-15\n",
    "3,Ravi,10,55000,2021-03-12\n",
    "4,Kiran,30,62000,2021-04-01\n",
    "5,Meena,20,58000,2021-05-18\n",
    "'''\n",
    "with open(\"emp.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "print(\"‚úÖ emp.csv created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476d67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================\n",
    "# 1Ô∏è‚É£ SETUP\n",
    "# =============================================================\n",
    "\n",
    "# --- Pandas ---\n",
    "import pandas as pd\n",
    "\n",
    "# --- PySpark ---\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, year, month, dayofmonth, dayofweek\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Pandas_vs_PySpark\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a950b",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37f00069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n",
      "   eno  ename  deptno  salary         doj\n",
      "0    1   Amit      10   50000  2021-01-10\n",
      "1    2   Neha      20   60000  2021-02-15\n",
      "2    3   Ravi      10   55000  2021-03-12\n",
      "3    4  Kiran      30   62000  2021-04-01\n",
      "4    5  Meena      20   58000  2021-05-18\n",
      "PySpark:\n",
      "+---+-----+------+------+----------+\n",
      "|eno|ename|deptno|salary|       doj|\n",
      "+---+-----+------+------+----------+\n",
      "|  1| Amit|    10| 50000|2021-01-10|\n",
      "|  2| Neha|    20| 60000|2021-02-15|\n",
      "|  3| Ravi|    10| 55000|2021-03-12|\n",
      "|  4|Kiran|    30| 62000|2021-04-01|\n",
      "|  5|Meena|    20| 58000|2021-05-18|\n",
      "+---+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df = pd.read_csv(\"emp.csv\")\n",
    "print(\"Pandas:\")\n",
    "print(pd_df.head())\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df = spark.read.csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "print(\"PySpark:\")\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a136ac",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Select specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb62022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n",
      "   eno  deptno\n",
      "0    1      10\n",
      "1    2      20\n",
      "2    3      10\n",
      "3    4      30\n",
      "4    5      20\n",
      "PySpark:\n",
      "+---+------+\n",
      "|eno|deptno|\n",
      "+---+------+\n",
      "|  1|    10|\n",
      "|  2|    20|\n",
      "|  3|    10|\n",
      "|  4|    30|\n",
      "|  5|    20|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df2 = pd.read_csv(\"emp.csv\", usecols=['eno','deptno'])\n",
    "print(\"Pandas:\")\n",
    "print(pd_df2.head())\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df2 = spark.read.csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "spark_df2 = spark_df2.select(\"eno\",\"deptno\")\n",
    "print(\"PySpark:\")\n",
    "spark_df2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6153dd",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387d4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "\n",
    "# --- Pandas ---\n",
    "pd_df3 = pd.read_csv(\"emp.csv\", dtype={'eno': int, 'salary': float})\n",
    "print(\"Pandas:\")\n",
    "print(pd_df3.dtypes)\n",
    "\n",
    "# --- PySpark ---\n",
    "schema = StructType([\n",
    "    StructField(\"eno\", IntegerType(), True),\n",
    "    StructField(\"ename\", StringType(), True),\n",
    "    StructField(\"deptno\", IntegerType(), True),\n",
    "    StructField(\"salary\", FloatType(), True),\n",
    "    StructField(\"doj\", StringType(), True)\n",
    "])\n",
    "spark_df3 = spark.read.csv(\"emp.csv\", header=True, schema=schema)\n",
    "print(\"PySpark:\")\n",
    "spark_df3.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f924ce27",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Skipping rows / partial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16174e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas (rows 3‚Äì4):\n",
      "   2   Neha  20  60000  2021-02-15\n",
      "0  3   Ravi  10  55000  2021-03-12\n",
      "1  4  Kiran  30  62000  2021-04-01\n",
      "PySpark (first 2 rows):\n",
      "+---+-----+------+------+----------+\n",
      "|eno|ename|deptno|salary|       doj|\n",
      "+---+-----+------+------+----------+\n",
      "|  1| Amit|    10| 50000|2021-01-10|\n",
      "|  2| Neha|    20| 60000|2021-02-15|\n",
      "+---+-----+------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df4 = pd.read_csv(\"emp.csv\", skiprows=2, nrows=2)\n",
    "print(\"Pandas (rows 3‚Äì4):\")\n",
    "print(pd_df4)\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df4 = spark.read.csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "spark_df4 = spark_df4.limit(2)\n",
    "print(\"PySpark (first 2 rows):\")\n",
    "spark_df4.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232c4724",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Parsing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27bcf2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas date parts:\n",
      "0    2021\n",
      "1    2021\n",
      "2    2021\n",
      "3    2021\n",
      "4    2021\n",
      "Name: doj, dtype: int32\n",
      "0    1\n",
      "1    2\n",
      "2    3\n",
      "3    4\n",
      "4    5\n",
      "Name: doj, dtype: int32\n",
      "PySpark date parts:\n",
      "+----------+----+-----+---+-------+\n",
      "|       doj|year|month|day|weekday|\n",
      "+----------+----+-----+---+-------+\n",
      "|2021-01-10|2021|    1| 10|      1|\n",
      "|2021-02-15|2021|    2| 15|      2|\n",
      "|2021-03-12|2021|    3| 12|      6|\n",
      "|2021-04-01|2021|    4|  1|      5|\n",
      "|2021-05-18|2021|    5| 18|      3|\n",
      "+----------+----+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df5 = pd.read_csv(\"emp.csv\", parse_dates=['doj'])\n",
    "print(\"Pandas date parts:\")\n",
    "print(pd_df5['doj'].dt.year.head())\n",
    "print(pd_df5['doj'].dt.month.head())\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df5 = spark.read.csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "spark_df5 = spark_df5.withColumn(\"doj\", to_date(\"doj\", \"yyyy-MM-dd\"))                     .withColumn(\"year\", year(\"doj\"))                      .withColumn(\"month\", month(\"doj\"))                      .withColumn(\"day\", dayofmonth(\"doj\"))                      .withColumn(\"weekday\", dayofweek(\"doj\"))\n",
    "print(\"PySpark date parts:\")\n",
    "spark_df5.select(\"doj\",\"year\",\"month\",\"day\",\"weekday\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63304ddc",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Handling Header & Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7e42f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas:\n",
      "   E_No E_Name  Dept_No    Sal         DOJ\n",
      "0     1   Amit       10  50000  2021-01-10\n",
      "1     2   Neha       20  60000  2021-02-15\n",
      "2     3   Ravi       10  55000  2021-03-12\n",
      "3     4  Kiran       30  62000  2021-04-01\n",
      "4     5  Meena       20  58000  2021-05-18\n",
      "PySpark:\n",
      "+----+------+-------+------+----------+\n",
      "|E_No|E_Name|Dept_No|   Sal|       DOJ|\n",
      "+----+------+-------+------+----------+\n",
      "| eno| ename| deptno|salary|       doj|\n",
      "|   1|  Amit|     10| 50000|2021-01-10|\n",
      "|   2|  Neha|     20| 60000|2021-02-15|\n",
      "|   3|  Ravi|     10| 55000|2021-03-12|\n",
      "|   4| Kiran|     30| 62000|2021-04-01|\n",
      "|   5| Meena|     20| 58000|2021-05-18|\n",
      "+----+------+-------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df6 = pd.read_csv(\"emp.csv\", header=0, names=['E_No','E_Name','Dept_No','Sal','DOJ'])\n",
    "print(\"Pandas:\")\n",
    "print(pd_df6.head())\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df6 = spark.read.csv(\"emp.csv\", header=False, inferSchema=True)\n",
    "spark_df6 = spark_df6.toDF(\"E_No\",\"E_Name\",\"Dept_No\",\"Sal\",\"DOJ\")\n",
    "print(\"PySpark:\")\n",
    "spark_df6.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a4ed0",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Encoding & Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73abe63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df7 = pd.read_csv(\"emp.csv\", encoding='utf-8')\n",
    "print(\"Pandas read with encoding:\")\n",
    "print(pd_df7.head())\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df7 = spark.read.option(\"encoding\",\"UTF-8\").csv(\"emp.csv\", header=True, inferSchema=True)\n",
    "print(\"PySpark read with encoding:\")\n",
    "spark_df7.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b993185e",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Writing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d690a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pandas written file: output_pandas.csv\n",
      "‚úÖ PySpark written folder: output_spark/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Pandas ---\n",
    "pd_df.to_csv(\"output_pandas.csv\", index=False)\n",
    "print(\"‚úÖ Pandas written file: output_pandas.csv\")\n",
    "\n",
    "# --- PySpark ---\n",
    "spark_df.write.option(\"header\", True).mode(\"overwrite\").csv(\"output_spark\")\n",
    "print(\"‚úÖ PySpark written folder: output_spark/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39758ea",
   "metadata": {},
   "source": [
    "\n",
    "# ‚úÖ Summary Table\n",
    "\n",
    "| Feature | Pandas | PySpark |\n",
    "|----------|---------|---------|\n",
    "| Read CSV | `pd.read_csv()` | `spark.read.csv()` |\n",
    "| Select Columns | `usecols=['A','B']` | `.select(\"A\",\"B\")` |\n",
    "| Dtype | `dtype={'A':int}` | `schema=StructType([...])` |\n",
    "| Skip Rows | `skiprows=10` | No direct ‚Äî filter manually |\n",
    "| nrows | `nrows=10` | `.limit(10)` |\n",
    "| Parse Dates | `parse_dates=['col']` | `to_date()` + date funcs |\n",
    "| Header | `header=1` | `.option(\"header\",True)` |\n",
    "| Names | `names=[...]` | `.toDF(\"col1\",\"col2\")` |\n",
    "| Encoding | `encoding='utf-8'` | `.option(\"encoding\",\"utf-8\")` |\n",
    "| Compression | `compression='gzip'` | auto |\n",
    "| Write CSV | `to_csv(index=False)` | `.write.option(\"header\",True).csv()` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f00c8ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PySpark version: 4.0.0\n",
      "‚úÖ Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"‚úÖ PySpark version:\", pyspark.__version__)\n",
    "print(\"‚úÖ Spark version:\", spark.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04973ce",
   "metadata": {},
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005e893",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
